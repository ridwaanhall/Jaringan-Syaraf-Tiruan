# -*- coding: utf-8 -*-
"""Perceptron.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KunhAdwEBiYFTHddomoMpOGJTwmKt8yu
"""


# %%
!gdown --folder https://drive.google.com/drive/folders/14Lu-2zt-TAenXlykysJoFkvAME_S1DiV

# %%
import warnings
warnings.filterwarnings("ignore")

# %%
# Tidak perlu dirubah
import torch
from torch import nn

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

"""## Data

### Data experiment

##Data Preprocessing
"""
# %%
from torch.utils.data import DataLoader, Dataset, TensorDataset

class TitanicDataset:
    def __init__(self, data, data_prep_version='1'):
        self.data_prep_version = data_prep_version
        self.data = self._data_preprocessing(data)

    def _data_preprocessing(self, data):
        if self.data_prep_version == '1':
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()

            out = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)
            out['Sex'] = le.fit_transform(out['Sex'])
            out['Embarked'] = le.fit_transform(out['Embarked'])
            return out

        elif self.data_prep_version == '2':
            dummy_fields=['Sex', 'Embarked']
            for each in dummy_fields:
                dummies= pd.get_dummies(data[each], prefix= each, drop_first=False)
                data = pd.concat([data, dummies], axis=1)

            fields_to_drop=['PassengerId', 'Cabin', 'Name', 'Sex', 'Ticket', 'Embarked']
            out = data.drop(fields_to_drop, axis=1)
            return out

        elif self.data_prep_version == '3':
            dummy_fields=['Sex', 'Embarked']
            for each in dummy_fields:
                dummies= pd.get_dummies(data[each], prefix= each, drop_first=False)
                data = pd.concat([data, dummies], axis=1)

            fields_to_drop=['PassengerId', 'Cabin', 'Name', 'Sex', 'Ticket', 'Embarked']
            out = data.drop(fields_to_drop, axis=1)
            out['FamilySize'] = out['SibSp'] + out['Parch'] + 1
            return out

        else:
            pass

    def as_df(self):
        return self.data

# %%
import pandas as pd

df_train = pd.read_csv('/content/Perceptron Challenge Dataset/train_data_cl_v2.csv')
df_test = pd.read_csv('/content/Perceptron Challenge Dataset/test_data_cl_v2.csv')
df_gt = pd.read_csv('/content/Perceptron Challenge Dataset/test_data_GroundTruth_cl.csv')
df_test = pd.concat([df_gt, df_test], axis=1)


df_train.head()

data_version = 1

df_train_final = TitanicDataset(df_train, str(data_version)).as_df()
df_test_final = TitanicDataset(df_test, str(data_version)).as_df()

df_train_final.head()

"""## Model

[Activation Function](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)
"""

# %%
import torch
from torch import nn

torch.manual_seed(42)

class Perceptron(nn.Module):
    def __init__(self,
                 in_features: int,
                 out_features: int,
                 bias: bool =True):
        super().__init__()
        self.node = nn.Linear(
            in_features=in_features,
            out_features=out_features,
            bias=bias
        )
        # self.act_func = nn.Sigmoid()
        self.act_func = nn.ReLU()
        # self.act_func = nn.LeakyReLU()
        # self.act_func = nn.Tanh()


    def forward(self, x):
        out = self.node(x)
        return self.act_func(out)

"""## Training and testing"""

# %%
import os
from tqdm.auto import tqdm

torch.manual_seed(42)

def accuracy(logits, target, t):
    logits[logits < t] = 0
    logits[logits >= t] = 1
    return sum(logits == target) / len(target)

def train(model,
          train_dataloader,
          test_dataloader,
          loss_fn,
          optimizer,
          lr_scheduler=None,
          epochs=20,
          show_progress=True,
          device='cpu'):

    config = {
        'activation': model.act_func.__class__.__name__,
        'loss_function': loss_fn.__class__.__name__,
        'optimizer': optimizer.__class__.__name__,
        'batch_size': batch_size,
        'epochs': epochs,
        'learning_rate': optimizer.param_groups[0]['lr'],
        'lr_scheduler_step': scheduler.step_size
    }

    result = {
        'train_loss': [],
        'train_acc': [],
        'test_loss': [],
        'test_acc': []
    }

    for epoch in tqdm(range(epochs)):
        train_loss, train_acc = 0, 0

        model.to(device)
        # train mode
        model.train()
        for data in train_dataloader:
            # move data to available device (cpu/cuda)
            X, y = data[:, 1:].to(device), data[:, 0].to(device)

            # forward
            logits = model(X).squeeze(1)

            # backward
            loss = loss_fn(logits, y) # error
            optimizer.zero_grad() # reset gradient
            loss.backward() # count gradient
            optimizer.step() # perform update weights

            # accuracy
            acc = accuracy(logits, y, 0.5)

            # acumulate loss and accuracy
            train_loss += loss.item()
            train_acc += acc.item()

        test_loss, test_acc = 0, 0

        # eval mode
        model.eval()
        for data_test in test_dataloader:
            X_test, y_test = data_test[:, 1:].to(device), data_test[:, 0].to(device)

            with torch.inference_mode():
                test_logits = model(X_test).squeeze(1)
                test_loss += loss_fn(test_logits, y_test).item()
                test_acc += accuracy(test_logits, y_test, 0.5).item()

        train_loss, train_acc = train_loss / len(train_dataloader), train_acc / len(train_dataloader)
        test_loss, test_acc = test_loss / len(test_dataloader), test_acc / len(test_dataloader)

        if lr_scheduler is not None:
            lr_scheduler.step()

        result['train_loss'].append(train_loss)
        result['train_acc'].append(train_acc)
        result['test_acc'].append(test_acc)
        result['test_loss'].append(test_loss)

        if (epoch + 1) % 1 == 0 and show_progress:
            print(f"Epoch: {epoch+1:4d}/{epochs}",
                f"| Train Loss: {train_loss:.4f}",
                f"| Train Acc: {train_acc:.3%}",
                f"| Test Loss: {test_loss:.4f}",
                f"| Test Acc: {test_acc:.3%}"
            )

        torch.save(model.state_dict(), f'models/model_{epoch + 1}.pth')

    return result, config

# %%
batch_size = 16
epochs = 20
lr = 0.01
lr_scheduler_step = 4

# %%
torch.manual_seed(42)

df_train_final = TitanicDataset(df_train, str(data_version)).as_df()
df_test_final = TitanicDataset(df_test, str(data_version)).as_df()

train_ds = torch.Tensor(df_train_final.values)
test_ds = torch.Tensor(df_test_final.values)

train_dataloader = DataLoader(
    train_ds,
    batch_size=batch_size,
    shuffle=True,
    num_workers=2
)

test_dataloader = DataLoader(
    test_ds,
    batch_size=batch_size,
    num_workers=2
)

len(train_dataloader), len(test_dataloader)

model = Perceptron(df_train_final.shape[-1]-1, 1, bias=True).to(device)
os.makedirs('models', exist_ok=True)
torch.manual_seed(42)

loss_fn = nn.MSELoss() # error
optimizer = torch.optim.Adam(params=model.parameters() , lr=lr) # update wights
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_scheduler_step, gamma=0.1) # scale down lr

result, config = train(
    model=model,
    train_dataloader=train_dataloader,
    test_dataloader=test_dataloader,
    epochs=epochs,
    loss_fn=loss_fn,
    optimizer=optimizer,
)

"""### Plot"""

# %%
import matplotlib.pyplot as plt
import numpy as np

def plot_loss_and_acc(result, dir):

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))

    # Loss subplot
    ax1.plot(result['train_loss'], label='Train Loss')
    ax1.plot(result['test_loss'], label='Test Loss')
    ax1.set_title('Train')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Loss')
    ax1.legend()

    # Accuracy subplot
    ax2.plot(result['train_acc'], label='Train Accuracy')
    ax2.plot(result['test_acc'], label='Test Accuracy')
    ax2.set_title('Test')
    ax2.set_xlabel('Epochs')
    ax2.set_ylabel('Accuracy')
    ax2.legend()

    fig.tight_layout()
    fig.savefig(f'{dir}/result.jpg', format='jpg')
    plt.show()

def plot_feature_importances(model, df_train, dir):

    weights = model.node.weight.softmax(1).squeeze(0).cpu().detach().numpy()
    columns = df_train.columns[1:]

    # Sort features by importance
    items = dict(sorted(({c: w for c, w in zip(columns, weights)}).items(),
                       key=lambda x: x[-1],
                       reverse=True))

    # Create bar plot
    plt.barh(list(items.keys()), items.values(), color='blue')
    plt.xlabel("Weight Magnitude")
    plt.gca().invert_yaxis()
    plt.title("Feature Importances")
    plt.tight_layout()
    plt.savefig(f"{dir}/feature_important.jpg", format='jpg', bbox_inches='tight')

    plt.show()

"""### Push to hf

[link token](https://huggingface.co/settings/tokens)
"""
# %%
from huggingface_hub import login
login()

# Jangan diubah
# %%
import os
import json
import shutil
from datetime import datetime, timedelta
from huggingface_hub import HfApi

dir = f'dataV{data_version}/' + '/'.join([f'{k}_{v}' for k, v in config.items()])

os.makedirs(dir, exist_ok=True)

result_object = json.dumps(result, indent=4)
config_object = json.dumps(config, indent=4)

with open(f"{dir}/result.json", "w") as outfile:
    outfile.write(result_object)

with open(f"{dir}/config.json", "w") as outfile:
    outfile.write(config_object)

plot_loss_and_acc(result, dir)
plot_feature_importances(model, df_train_final, dir)

if os.path.exists(f'{dir}/models'):
    shutil.rmtree(f'{dir}/models')

shutil.copytree('models', f'{dir}/models')

api = HfApi()
api.upload_folder(
    folder_path=f'dataV{data_version}',
    path_in_repo=f'dataV{data_version}',
    repo_id="JST4/Experiment",
    repo_type="dataset",
)

print(dir)

"""### Model TF

"""
# %%
X_train_tf = df_train_final.drop(['Survived'], axis=1)
y_train_tf = df_train_final['Survived']

import tensorflow as tf

model = tf.keras.models.Sequential(
    tf.keras.layers.Dense(units=1, activation='sigmoid')
)

loss = tf.keras.losses.BinaryCrossentropy()
optimizer = tf.keras.optimizers.Adam(learning_rate=0.03)

model.compile(optimizer=optimizer,
              loss=loss,
              metrics=['accuracy'])

model.fit(X_train_tf, y_train_tf, batch_size=16, epochs=20)

model.weights